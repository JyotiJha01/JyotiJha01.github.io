{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ðŸ§  Recurrent Neural Networks (RNN)\n\n## ðŸ”¹ What is an RNN?\n- A **Recurrent Neural Network (RNN)** is a type of neural network designed to handle **sequential data** (data where order matters).\n- Unlike Feedforward Neural Networks, RNNs have a **memory** that allows them to keep track of **previous inputs** when making predictions.\n\n---\n\n## ðŸ”¹ Why do we need RNN?\nExample:  \n- Sentence: *\"I am going to the ...\"*  \n- Because of memory, you can guess the next word might be *\"park\"* or *\"store\"*.  \n- A simple neural network without memory cannot do this, but an RNN can.\n\n---\n\n## ðŸ”¹ How does an RNN work?\nAt each time step **t**, the RNN takes:\n- Current input: \\(x_t\\)  \n- Previous hidden state: \\(h_{t-1}\\)  \n\nAnd produces:\n- New hidden state: \\(h_t\\) (updated memory)  \n- Output: \\(y_t\\)  \n\n---\n\n## ðŸ”¹ Mathematical Representation\nAt time step **t**:\n\n\\[\nh_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\n\\]\n\n\\[\ny_t = g(W_{hy}h_t + b_y)\n\\]\n\nWhere:  \n- \\(h_t\\) = hidden state (memory at time t)  \n- \\(x_t\\) = input at time t  \n- \\(W\\) = weight matrices  \n- \\(f, g\\) = activation functions (e.g., tanh, softmax)  \n\n---\n\n## ðŸ”¹ Flow Example\nPredicting the next word:\n1. Input = \"I\" â†’ Hidden state remembers **\"I\"** â†’ Output: \"am\"\n2. Input = \"am\" â†’ Hidden state remembers **\"I am\"** â†’ Output: \"going\"\n3. Input = \"going\" â†’ Hidden state remembers **\"I am going\"** â†’ Output: \"to\"\n\n---\n\n  \n","metadata":{}},{"cell_type":"markdown","source":"## RNN Code\n\nðŸ”¹ What this does\n\nDefines random weights (Wxh, Whh, Why)\n\nInitializes hidden state h = 0\n\nRuns through a sequence of inputs (x1, x2, x3)\n\nAt each step:\n\n\n* Updates hidden state using tanh\n* Computes output\n* Prints results\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Parameters\ninput_size = 3   # input vector length\nhidden_size = 2  # hidden state length\n\n# Random weights (for demo)\nWxh = np.random.randn(hidden_size, input_size)   # input -> hidden\nWhh = np.random.randn(hidden_size, hidden_size)  # hidden -> hidden\nWhy = np.random.randn(1, hidden_size)            # hidden -> output\n\n# Initial hidden state\nh = np.zeros((hidden_size, 1))\n\n# Example input sequence (3 one-hot vectors)\nsequence = [\n    np.array([[1], [0], [0]]),  # word1\n    np.array([[0], [1], [0]]),  # word2\n    np.array([[0], [0], [1]])   # word3\n]\n\n# Forward pass through sequence\nfor t, x in enumerate(sequence, 1):\n    h = np.tanh(Wxh @ x + Whh @ h)   # update hidden state\n    y = Why @ h                      # compute output\n    print(f\"Step {t}:\")\n    print(\" Hidden state:\", h.ravel())\n    print(\" Output:\", y.ravel(), \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T04:48:51.296589Z","iopub.execute_input":"2025-08-26T04:48:51.297553Z","iopub.status.idle":"2025-08-26T04:48:51.340169Z","shell.execute_reply.started":"2025-08-26T04:48:51.297515Z","shell.execute_reply":"2025-08-26T04:48:51.338862Z"}},"outputs":[{"name":"stdout","text":"Step 1:\n Hidden state: [0.59026496 0.77501423]\n Output: [-2.09563825] \n\nStep 2:\n Hidden state: [0.66654795 0.81645259]\n Output: [-2.27501764] \n\nStep 3:\n Hidden state: [-0.95474594  0.18505552]\n Output: [1.14916579] \n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## ðŸ”¹ Problems with Simple RNN\n- **Vanishing Gradient**: Forgetting long-term dependencies in long sequences.  \n- **Exploding Gradient**: Gradients can grow too large during training.  \n\nSolutions â†’ **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)**.  \n\n---\n\n###  In Short RNN:\n- RNN = Neural network with memory.  \n- Best for **sequential data**: text, speech, time-series.  \n- Struggles with **long-term dependencies** â†’ solved by LSTM/GRU.","metadata":{}}]}